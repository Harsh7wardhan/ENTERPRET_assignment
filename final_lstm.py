# -*- coding: utf-8 -*-
"""FINAL_LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dULmwucOJwG5w_FK8YT6V3stjoM4TAfo
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import pickle

train_data=pd.read_csv("C:/Users/harsh/Downloads/train.csv")
test_data=pd.read_csv("C:/Users/harsh/Downloads/test.csv")

train_data.isnull().any().sum()

test_data.isnull().any().sum()

"""THERE ARE NO NULL VALUES"""

sns.countplot(train_data['label'])

"""0 - NEGATIVE 1 - NEUTRAL 2 - POSITIVE

Here , we eliminate all records having label = 1 , that is a NEUTRAL SENTIMENT response whiich is of no use to us as mentioned in the motivation for this project.
"""

new_train=train_data[train_data['label']!=1]

sns.countplot(new_train['label'])

new_train.info()

import tensorflow as tf
tf.__version__

messages=new_train.copy()

messages.reset_index(inplace=True)

messages.head()

from tensorflow.keras.layers import Embedding
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing.text import one_hot
from tensorflow.keras.layers import LSTM
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Bidirectional
from tensorflow.keras.layers import Dropout

### Vocabulary size
voc_size=5000

import nltk
nltk.download('punkt')
nltk.download('stopwords')

# Cleaning the texts
import re
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer

from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
import re
ps = PorterStemmer()
corpus = []
for i in range(0, len(messages)):
    review = re.sub('[^a-zA-Z]', ' ', messages['text'][i])
    review = review.lower()
    review = review.split()
    
    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]
    review = ' '.join(review)
    corpus.append(review)

onehot_repr=[one_hot(words,voc_size)for words in corpus] 
onehot_repr

"""Embedding Representation"""

sent_length=20
embedded_docs=pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)
print(embedded_docs)

embedded_docs[0]

## Creating model
embedding_vector_features=45
model=Sequential()
model.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length))
model.add(LSTM(128,input_shape=(embedded_docs.shape),activation='relu',return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(128,activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(32,activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(4,activation='softmax'))
model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
print(model.summary())

## Creating model
embedding_vector_features=40
model1=Sequential()
model1.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length))
model1.add(Bidirectional(LSTM(100)))
model1.add(Dropout(0.3))
model1.add(Dense(1,activation='sigmoid'))
model1.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
print(model1.summary())

y=new_train['label']

len(embedded_docs),y.shape

import numpy as np
X_final=np.array(embedded_docs)
y_final=np.array(y)

X_final.shape,y_final.shape

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.25, random_state=0)

"""Model Training"""

### Finally Training
model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=20,batch_size=64)

"""Performance Metrics And Accuracy"""

y_pred1=model.predict(X_test)
classes_x=np.argmax(y_pred1,axis=1)
classes_x

from sklearn.metrics import confusion_matrix

confusion_matrix(y_test,classes_x)

from sklearn.metrics import accuracy_score
accuracy_score(y_test,classes_x)

"""The accuracy using  LSTM/Bidirectional LSTM was not great as compared to Bag of words model . I was able to get as high as **76.07%**"""

from sklearn.metrics import classification_report
print(classification_report(y_test,classes_x))

from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer

ps = PorterStemmer()
corpus_new = []
for i in range(0, len(test_data)):
    review_new = re.sub('[^a-zA-Z]', ' ', test_data['text'][i])
    review_new = review_new.lower()
    review_new = review_new.split()

    review_new = [ps.stem(word) for word in review_new if not word in stopwords.words('english')]
    review_new = ' '.join(review_new)
    corpus_new.append(review_new)

onehot_repr=[one_hot(words,voc_size)for words in corpus_new]

sent_length=20
embedded_docs_new=pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)
print(embedded_docs_new)
X_final=np.array(embedded_docs)

y_pred_new=model.predict(X_final)
classes_x=np.argmax(y_pred1,axis=1)
classes_x


# Saving model to disk
pickle.dump(model, open('model_2.pkl','wb'))

# Loading model to compare the results
model_2 = pickle.load(open('model_2.pkl','rb'))


